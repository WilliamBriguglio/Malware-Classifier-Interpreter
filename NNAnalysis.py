import sys
import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix
from sklearn.metrics import balanced_accuracy_score
from sklearn.preprocessing import LabelEncoder
from keras.optimizers import Adam
from keras.models import model_from_json
from keras.utils import np_utils
import innvestigate


print("Loading data...")
x_test = pd.read_csv("MITestArr.csv", header=None, dtype=int).to_numpy(dtype=int)
y_test_1 = pd.read_csv("MITestLbs.csv", header=None, dtype=int).to_numpy(dtype=int) #labels in range 1-9
encoder = LabelEncoder()
encoder.fit(y_test_1)
y_test_0 = encoder.transform(y_test_1) #make labels in range 0-8
y_test_OHE = np_utils.to_categorical(y_test_0) #make one hot encoded label vectors
print("Data loaded!")

print("Loading Model")
json_file = open('NNmodelBest.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
model = model_from_json(loaded_model_json)
model.load_weights("NNweightsBest.hdf5")
print("Loaded model from disk")

model.compile(loss="categorical_crossentropy",
		optimizer=Adam(lr=0.001, epsilon=0.0001),
		metrics=["accuracy"])

#get model predictions
y_pred = model.predict_classes(x_test)

#get and display confusion matrix, accuracy, loss, and weighted accuracy
cm = confusion_matrix(y_test_0, y_pred)
weightedAcc = balanced_accuracy_score(y_test_0, y_pred)
scores = model.evaluate(x_test, y_test_OHE, verbose=0)
print("Loss: ",scores[0],"\taccuracy: ",scores[1],"\tweighted accuracy: ",weightedAcc)
print("Confusion Matrix:")
print(cm)


#initialize analyzer object in LRP-epsilon mode
analyzer = innvestigate.create_analyzer("lrp.epsilon", model)

#select samples for analysis
if len(sys.argv) == 1: #if no cmd line argument, select all classes
	n = len(x_test)
	s = 0
	test_samples = list(zip(x_test[s:n], y_test_OHE[s:n]))
	print("Averaging absolute relevances across all classes...")
else: #if cmd line arguments, select samples where true class == sys.argvp[1]
	
	#get number of samples with true class = sys.argv[1]
	unique, counts = np.unique(y_test_1, return_counts=True)
	calssDict = dict(zip(unique, counts))
	c = calssDict[int(sys.argv[1])]
	num_classes = len(calssDict)

	x_coi = np.zeros((c, x_test.shape[1]))
	y_coi = np.zeros((c, num_classes))
	j=0 #index in new array
	for i in range(len(y_test_1)):
		if y_test_1[i,0] == int(sys.argv[1]): #if sample belongs to target class
			x_coi[j] = x_test[i]
			y_coi[j] = y_test_OHE[i]
			j+=1
	
	test_samples = list(zip(x_coi[:], y_coi[:]))
	print("Averaging relevances across class "+sys.argv[1]+" (# of samples = "+str(c)+")...")
	
#run analysis for all samples in test_samples

#create array to store anlysys results
analysis = np.zeros([len(test_samples), x_test.shape[1]])

#for each sample in test samples
for i, (x, y) in enumerate(test_samples):
	x = x[None, :]

	#get relevances of first layer
	a = analyzer.analyze(x)
	a = a.astype(np.float32)
    
	#store analysis for sample i
	analysis[i]  = a[0]

#Save analysis results
if len(sys.argv) == 1: #if all classes selected

	#get means of absolute relevances
	analysis = np.absolute(analysis)
	mean_rel = np.mean(analysis, axis=0)

	#save mean absolute relevances of all input nodes
	with open("Abs_Mean_All.csv", "w") as fp:
		for a in mean_rel:
			fp.write(str(a)+"\n")

	#save 50 highest mean absolute relevances
	max = np.argsort(mean_rel)
	max = max[-50:]
	with open("Abs_Max_50.csv", "w") as fp:
		for a in max:
			fp.write(str(mean_rel[a])+","+str(a)+"\n")

else: #if only one class selected

	#get mean relevances
	mean_rel = np.mean(analysis, axis=0)
	
	#save mean relevances of all input nodes
	with open(sys.argv[1]+"_Mean_All.csv", "w") as fp:
		for a in mean_rel:
			fp.write(str(a)+"\n")

	#save 50 highest mean relevances
	max = np.argsort(mean_rel)
	max = max[-50:]
	with open(sys.argv[1]+"_Max_50.csv", "w") as fp:
		for a in max:
			fp.write(str(mean_rel[a])+","+str(a)+"\n")

print("\n\n\t\t\t\tDONE!")



