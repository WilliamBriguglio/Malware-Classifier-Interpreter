import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import class_weight
import keras.models
from keras.callbacks import ModelCheckpoint
from keras.optimizers import Adam
from keras.models import model_from_json
from keras.initializers import TruncatedNormal as TN
from keras.utils import np_utils


print("Loading data...")
#get training data
x_train = pd.read_csv("MITrainArr.csv", header=None, dtype=int).to_numpy(dtype=int)
y_train_1 = pd.read_csv("MITrainLbs.csv", header=None, dtype=int).to_numpy(dtype=int) #labels in range 1-9
encoder = LabelEncoder()
encoder.fit(y_train_1)
y_train_0 = encoder.transform(y_train_1) #make labels in range 0-8
weights = class_weight.compute_class_weight('balanced', np.unique(y_train_0), y_train_0) #get weights to balance accuracy score to account for class imbalances
weights = dict(enumerate(weights))
y_train_OHE = np_utils.to_categorical(y_train_0) #make one hot encoded label vectors

#get test data
x_test = pd.read_csv("MITestArr.csv", header=None, dtype=int).to_numpy(dtype=int)
y_test_1 = pd.read_csv("MITestLbs.csv", header=None, dtype=int).to_numpy(dtype=int) #labels in range 1-9
encoder = LabelEncoder() 
encoder.fit(y_test_1)
y_test_0 = encoder.transform(y_test_1) #make labels in range 0-8
y_test_OHE = np_utils.to_categorical(y_test_0) #make one hot encoded label vectors
print("Data loaded!")


print("Training model...")

#create model
model = keras.models.Sequential([
    keras.layers.Dense(40, activation="tanh", kernel_initializer=TN(stddev=0.1), use_bias=False),
    keras.layers.Dense(9, activation="sigmoid", kernel_initializer=TN(stddev=0.1), use_bias=False),
])

#compile model
model.compile(loss="categorical_crossentropy",
		optimizer=Adam(lr=0.001, decay=0.0, amsgrad=True),
		weighted_metrics=["categorical_accuracy"])

#save model weights with lowerst validation loss
mcp_save = ModelCheckpoint('NNweightsBest.hdf5', save_best_only=True, monitor='val_loss', mode='min')

#train model
history = model.fit(x_train, y_train_OHE,
		    batch_size=300,
		    shuffle=True,
		    epochs=100,
		    verbose=1,
		    class_weight=weights,
		    validation_data=(x_test, y_test_OHE),
		    callbacks=[mcp_save])
			
print("Model trained!")

#save model architecture
model_json = model.to_json()
with open("NNmodelBest.json", "w") as json_file:
    json_file.write(model_json)

#load best weights and model arch
json_file = open('NNmodelBest.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
model = model_from_json(loaded_model_json)
model.load_weights("NNweightsBest.hdf5")
model.compile(loss="categorical_crossentropy",
		optimizer=Adam(lr=0.001, epsilon=0.0001),
		metrics=["accuracy"])

#evaluate model
y_pred = model.predict_classes(x_test) #get model predictions
cm = confusion_matrix(y_test_0, y_pred)
scores = model.evaluate(x_test, y_test_OHE, verbose=0)
print("Results on test set classification:")
print("Loss: ",scores[0],"\t accuracy:",scores[1])
print("Confusion Matrix:")
print(cm)
